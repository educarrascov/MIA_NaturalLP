{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b7e1f8-0c4d-426d-a0d6-f6fe6d719a5c",
   "metadata": {},
   "source": [
    "## **Eduardo Carrasco Vidal** <img src=\"img/logo.png\" align=\"right\" style=\"width: 120px;\"/>\n",
    "\n",
    "**Magister en Inteligencia Artificial, Universidad Adolfo Ibáñez.**\n",
    "\n",
    "**Profesor:** John Atkinson.\n",
    "**Curso:** Procesamiento del Leguaje Natural (Natural Language Processing).\n",
    "\n",
    "Enlace al repositorio del alumno en [GitHub](https://github.com/educarrascov/MIA_NaturalLP) _@educarrascov_\n",
    "\n",
    "![Python](https://img.shields.io/badge/python-%2314354C.svg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3065cf",
   "metadata": {
    "id": "Lu0zsf5HuOqR"
   },
   "source": [
    "**CLASIFICACIÓN DE SENTIMIENTOS CON LSA**\n",
    "\n",
    "Este programa utiliza  *Análisis Semántico Latente*  (LSA) para generar [embeddings](https://es.wikipedia.org/wiki/Word_embedding) que permitan realizar posteriormente una actividad de clasificación de sentimientos.\n",
    "\n",
    "La clasificación se realiza entrenando un modelo [Bayesiano Gausiano](https://iq.opengenus.org/gaussian-naive-bayes/).\n",
    "\n",
    "Primero, necesitamos instalar algunos paquetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c06c21-04ea-413c-8e58-c9f88fa84253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f7970",
   "metadata": {
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1632143158561,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "0KfmzU5tWWii"
   },
   "outputs": [],
   "source": [
    "import es_core_news_sm\n",
    "import os\n",
    "import pandas as pd\n",
    "import regex\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98e2e6",
   "metadata": {
    "id": "_KtZYqn4WZ8u"
   },
   "source": [
    "Definamos la función **CrearEspacioLSA(corpus,dim,NombreModelo)**, que  utiliza LSA para crear un *espacio semántico* de dimensiones reducidas (i.e., modelo vectorial que representa documentos y palabras), que se graba y luego puede ser cargado por otros programas. La función recibe la lista de documentos pre-procesados (**corpus**), el número de dimensiones a reducir (**dim**) vía [SVD](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491), y el nombre de la carpeta donde se grabará el modelo (**NombreModelo**).\n",
    "\n",
    "LSA utiliza la *descomposición de valores singulares* (SVD) para descomponer la matriz de frecuencias original vía **TfidfVectorizer** en tres matrices: $U$, $\\Sigma$ y $V^T$. Luego, las nuevas representaciones vectoriales en dimensiones reducidas (**dim**)  se reconstruyen  como:\n",
    "\n",
    "*   Representación de  términos: $U(dim) *  \\Sigma(dim)$\n",
    "\n",
    "*   Representación  de documentos: $\\Sigma(dim) * V^T(dim)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76370ad6",
   "metadata": {
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1632143167929,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "eCT-8_bXXJ-X"
   },
   "outputs": [],
   "source": [
    "def CrearModeloLSA(textos,dim,NombreModelo):\n",
    "  MatrizFrec = TfidfVectorizer()\n",
    "  tf = MatrizFrec.fit_transform(textos).T\n",
    "  U, Sigma, VT = svd(tf.toarray())\n",
    "  # Se realiza producto punto de matrices con las nuevas dimensiones\n",
    "  terms = np.dot(U[:,:dim], np.diag(Sigma[:dim]))\n",
    "  docs  = np.dot(np.diag(Sigma[:dim]), VT[:dim, :]).T \n",
    "  vocab = MatrizFrec.get_feature_names()\n",
    "  GrabarModeloLSA(NombreModelo, Sigma, terms, docs, vocab)\n",
    "\n",
    "def GrabarModeloLSA(NombreModelo,Sigma,terms,docs, vocab):\n",
    "   existe = os.path.isdir(NombreModelo)\n",
    "   if not existe:\n",
    "       os.mkdir(NombreModelo)\n",
    "   joblib.dump(Sigma,   NombreModelo +\"/\"+'sigma.pkl') \n",
    "   joblib.dump(terms,   NombreModelo +\"/\"+'terms.pkl') \n",
    "   joblib.dump(docs,    NombreModelo +\"/\"+'docs.pkl') \n",
    "   joblib.dump(vocab,   NombreModelo +\"/\"+'vocab.pkl') \n",
    "\n",
    "def CargarModeloLSA(NombreModelo):\n",
    "    terms   = joblib.load(NombreModelo+\"/\"+'terms.pkl')\n",
    "    vocab   = joblib.load(NombreModelo+\"/\"+'vocab.pkl')\n",
    "    modelo =  CrearDiccionario(terms,vocab)\n",
    "    return(modelo)\n",
    "def CrearDiccionario(lista_vectores,claves):\n",
    "   dicc = {}\n",
    "   for  v in range(0,len(claves)):\n",
    "      dicc[claves[v]] = lista_vectores[v]\n",
    "   return(dicc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1388504",
   "metadata": {
    "id": "ohFNag39i0bX"
   },
   "source": [
    "Un aspecto importante al utilizar LSA es determinar cuál es el número óptimo de dimensiones a reducir. Esto depende del tamaño del corpus utiizado para construir el modelo:\n",
    "\n",
    "1.   Si el tamaño es muy grande (i.e., varios GigaBytes de texto), lo adecuado es utilizar entre 200 a 300 dimensiones.\n",
    "2.   Si el tamaño  es pequeño, se puede elegir el número de dimensiones como el número de valores singulares de la matriz $\\Sigma$) que maximice la *importancia* (descartando el primer valor pues corelaciona con el largo del corpus). La importancia de un valor singular $x$ es simplemente $x^2$.\n",
    "\n",
    "Para visualizar la importancia de los valores singulares, podemos definir  la función **GraficarImportancia($\\Sigma$)**, que grafica los valores singulares (i.e., dimensiones) versus importancia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d5b2a13",
   "metadata": {
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1632143450389,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "Cr98HYUDjjbm"
   },
   "outputs": [],
   "source": [
    "def GraficarImportancia(Sigma):   \n",
    "    NumValores = np.arange(len(Sigma))\n",
    "    Importancia = [x**2 for x in Sigma]\n",
    "    plt.bar(NumValores,Importancia)\n",
    "    plt.ylabel('Importancia')\n",
    "    plt.xlabel('Valores Singulares')\n",
    "    plt.title('Importancia de Valores Singulares en SVD')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef79b0",
   "metadata": {
    "id": "ifJKRQJlhNlA"
   },
   "source": [
    "Luego, definimos algunas funciones utilitarias para \"limpieza\" y pre-procesamieto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b593fc4",
   "metadata": {
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1632144019054,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "NElqWSE4ijWn"
   },
   "outputs": [],
   "source": [
    " def PreProcesar(textos):\n",
    "    texto_limpio = []\n",
    "    for texto in textos:  \n",
    "        texto = Lematizar(texto)     \n",
    "        texto = EliminaNumeroYPuntuacion(texto)      \n",
    "        texto_limpio.append(texto)\n",
    "    return(texto_limpio)\n",
    "\n",
    "def Lematizar(oracion):\n",
    "   doc = nlp(oracion)\n",
    "   lemas = [token.lemma_ for token in doc]\n",
    "   return(\" \".join(lemas))  \n",
    "\n",
    "def EliminaNumeroYPuntuacion(oracion):\n",
    "    string_numeros = regex.sub(r'[\\”\\“\\¿\\°\\d+]','', oracion)\n",
    "    return ''.join(c for c in string_numeros if c not in punctuation)\n",
    "\n",
    "def Tokenizar(oracion):\n",
    "    doc = nlp(oracion)\n",
    "    tokens = [palabra.text for palabra in doc]\n",
    "    return(tokens)\n",
    "\n",
    "def CrearCorpus(path):\n",
    "  directorio = os.listdir(path)\n",
    "  corpus = []\n",
    "  doc_id = []  \n",
    "  for NombreArchivo  in directorio:\n",
    "     try:\n",
    "          texto = open(path+NombreArchivo,'r',encoding=\"utf-8\").read()\n",
    "          corpus.append(texto)\n",
    "          doc_id.append(NombreArchivo)\n",
    "     except IsADirectoryError:\n",
    "          texto = \"\"\n",
    "  return(corpus,doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dad95e",
   "metadata": {
    "id": "jKla5jkolKay"
   },
   "source": [
    "Una vez que generamos las nuevas representaciones vectoriales para documentos y términos, podríamos utilizar dichos  vectores para realizar diversas tareas tales como clustering, clasificación, comparación de documentos, etc.\n",
    "\n",
    "Como ejemplo, definamos la función **GraficarVectores(vocab,vectores)**, que toma vectores de documentos (o palabras), y el vocabulario, y grafica cada uno de los elementos en un gráfico 2-dimensional. Dado que los vectores contienen más de 2 dimensiones, en este ejemplo, sólo tomamos las 2 primeras. \n",
    "\n",
    "El gráfico le permitirá visualizar espacialmente la *cercanía* que existe entre documentos para posteriores análisis.\n",
    "\n",
    "Alternativamente, Ud. podría aplicar otros métodos de reducción dimensional para graficas las mejores 2 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1697008",
   "metadata": {
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1632143202214,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "3E1aQFpbmjTd"
   },
   "outputs": [],
   "source": [
    "def GraficarVectores(vocab,vectores):\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in vectores:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])   \n",
    "    plt.figure(figsize=(7, 7))   \n",
    "    plt.title(\"Distribución Espacial de Documentos\")\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(vocab[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fbd7a",
   "metadata": {
    "id": "11s2WiDpnGGl"
   },
   "source": [
    "Note que los vectores para documentos o palabras están indexados por posición y no por nombre, lo que podría dificultad el acceso. Para mejorar esto, podemos definir la función **CrearDiccionario(Vectores,vocabulario)**, que dado un conjunto de **vectores** (de documentos o palabras) y un **vocabulario**, genera un nuevo arreglo donde el índice es el nombre correspondiente: una palabra en el caso de vectores de palabras o un nombre de documento en el caso de vectores de documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f402f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObtenerEmbeddingOracion(modelo, dim, oracion):\n",
    "   Lista_enbeddings = []\n",
    "   Tokens = Tokenizar(oracion)\n",
    "   for w in Tokens:\n",
    "       # Verificar que la palabra w exista en el modelo\n",
    "       try:\n",
    "           modelo[w]\n",
    "       except KeyError:\n",
    "           continue\n",
    "       # Obtener vector de la palabra w\n",
    "       embedding = modelo[w]\n",
    "       Lista_enbeddings.append(embedding)\n",
    "   embedding_palabras = np.array(Lista_enbeddings)\n",
    "   if (len(embedding_palabras) > 0):\n",
    "        embedding_oracion = embedding_palabras.mean(axis=0)\n",
    "   else:\n",
    "        embedding_oracion = np.zeros(dim)\n",
    "   return(embedding_oracion) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fee028",
   "metadata": {
    "id": "bmoNbxoIn8YC"
   },
   "source": [
    "Ahora, realizamos nuestro programa principal donde ajustamos la ruta donde se encuentran los documentos del corpus e inicializamos los modelos de lenguaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6002ced",
   "metadata": {
    "executionInfo": {
     "elapsed": 2270,
     "status": "ok",
     "timestamp": 1632143210937,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "p4XxZz6JoD5I"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'es_core_news_sm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiscursosOriginales/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mes_core_news_sm\u001b[49m\u001b[38;5;241m.\u001b[39mload()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'es_core_news_sm' is not defined"
     ]
    }
   ],
   "source": [
    "PATH = \"DiscursosOriginales/\"\n",
    "nlp = es_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a44bb",
   "metadata": {
    "id": "ZZlnKNVY10-C"
   },
   "source": [
    "Creamos un corpus a partir de los documentos de la carpeta de *PATH*. Luego, creamos el espacio semántico utilizando LSA y lo grabamos para recuperarlo posteriormente. Por ahora podemos suponer que vamos a reducir el espacio original a $dim=3$ ($dim$ debe ser menor que el número de documentos): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "de20b13f",
   "metadata": {
    "executionInfo": {
     "elapsed": 1467,
     "status": "ok",
     "timestamp": 1632144025053,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "uG1arANj2T2t",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim =  200\n",
    "if os.path.isdir(\"mi_lsa\"):\n",
    "    modeloLSA = CargarModeloLSA(\"mi_lsa\")\n",
    "else:\n",
    "    corpus, lista_docs = CrearCorpus(PATH+'Train/')\n",
    "    textos  = PreProcesar(corpus)\n",
    "    CrearModeloLSA(textos,dim,\"mi_lsa\")\n",
    "    modeloLSA = CargarModeloLSA(\"mi_lsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "014c79a3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100178.txt',\n",
       " '100227.txt',\n",
       " '100454.txt',\n",
       " '101255.txt',\n",
       " '101415.txt',\n",
       " '101614.txt',\n",
       " '101766.txt',\n",
       " '102005.txt',\n",
       " '102351.txt',\n",
       " '102791.txt',\n",
       " '103122.txt',\n",
       " '103313.txt',\n",
       " '103511.txt',\n",
       " '134150.txt',\n",
       " '134658.txt',\n",
       " '134801.txt',\n",
       " '135653.txt',\n",
       " '135757.txt',\n",
       " '135814.txt',\n",
       " '136230.txt',\n",
       " '136280.txt',\n",
       " '136377.txt',\n",
       " '136426.txt',\n",
       " '137702.txt',\n",
       " '138176.txt',\n",
       " '148292.txt',\n",
       " '148522.txt',\n",
       " '148806.txt',\n",
       " '149988.txt',\n",
       " '150322.txt',\n",
       " '150341.txt',\n",
       " '150633.txt',\n",
       " '151570.txt',\n",
       " '151937.txt',\n",
       " '152081.txt',\n",
       " '152091.txt',\n",
       " '152168.txt',\n",
       " '152358.txt',\n",
       " '152471.txt',\n",
       " '152616.txt',\n",
       " '153144.txt',\n",
       " '153186.txt',\n",
       " '165020.txt',\n",
       " '165487.txt',\n",
       " '165648.txt',\n",
       " '71541.txt',\n",
       " '71862.txt',\n",
       " '72017.txt',\n",
       " '72231.txt',\n",
       " '72656.txt',\n",
       " '72692.txt',\n",
       " '73117.txt',\n",
       " '73202.txt',\n",
       " '73235.txt',\n",
       " '73260.txt',\n",
       " '73601.txt',\n",
       " '73739.txt',\n",
       " '73742.txt',\n",
       " '73766.txt',\n",
       " '74214.txt',\n",
       " '74598.txt',\n",
       " '75121.txt',\n",
       " '75615.txt',\n",
       " '76138.txt',\n",
       " '76163.txt',\n",
       " '76467.txt',\n",
       " '76480.txt',\n",
       " '76776.txt',\n",
       " '76780.txt',\n",
       " '77336.txt',\n",
       " '77420.txt',\n",
       " '77484.txt',\n",
       " '77522.txt',\n",
       " '77735.txt',\n",
       " '77784.txt',\n",
       " '77867.txt',\n",
       " '77982.txt',\n",
       " '78008.txt',\n",
       " '78087.txt',\n",
       " '78517.txt',\n",
       " '78838.txt',\n",
       " '79051.txt',\n",
       " '79087.txt',\n",
       " '79229.txt',\n",
       " '79291.txt',\n",
       " '79583.txt',\n",
       " '79747.txt',\n",
       " '79770.txt',\n",
       " '79773.txt',\n",
       " '80178.txt',\n",
       " '80552.txt',\n",
       " '80706.txt',\n",
       " '80840.txt',\n",
       " '81188.txt',\n",
       " '81762.txt',\n",
       " '81872.txt',\n",
       " '82001.txt',\n",
       " '82274.txt',\n",
       " '82440.txt',\n",
       " '82911.txt',\n",
       " '83275.txt',\n",
       " '83602.txt',\n",
       " '83787.txt',\n",
       " '83968.txt',\n",
       " '84355.txt',\n",
       " '84418.txt',\n",
       " '84486.txt',\n",
       " '84780.txt',\n",
       " '84898.txt',\n",
       " '85124.txt',\n",
       " '85228.txt',\n",
       " '85341.txt',\n",
       " '85692.txt',\n",
       " '85848.txt',\n",
       " '86296.txt',\n",
       " '86848.txt',\n",
       " '86936.txt',\n",
       " '87482.txt',\n",
       " '87594.txt',\n",
       " '88004.txt',\n",
       " '88165.txt',\n",
       " '88171.txt',\n",
       " '88412.txt',\n",
       " '88449.txt',\n",
       " '88473.txt',\n",
       " '88676.txt',\n",
       " '88707.txt',\n",
       " '88981.txt',\n",
       " '89221.txt',\n",
       " '89553.txt',\n",
       " '89652.txt',\n",
       " '89759.txt',\n",
       " '90064.txt',\n",
       " '90121.txt',\n",
       " '90188.txt',\n",
       " '90235.txt',\n",
       " '90358.txt',\n",
       " '90492.txt',\n",
       " '90815.txt',\n",
       " '90825.txt',\n",
       " '90964.txt',\n",
       " '91144.txt',\n",
       " '91220.txt',\n",
       " '91265.txt',\n",
       " '91320.txt',\n",
       " '92048.txt',\n",
       " '92280.txt',\n",
       " '92641.txt',\n",
       " '92796.txt',\n",
       " '93108.txt',\n",
       " '93252.txt',\n",
       " '93461.txt',\n",
       " '93782.txt',\n",
       " '93904.txt',\n",
       " '94277.txt',\n",
       " '94447.txt',\n",
       " '94637.txt',\n",
       " '94863.txt',\n",
       " '95077.txt',\n",
       " '96144.txt',\n",
       " '96419.txt',\n",
       " '96479.txt',\n",
       " '96769.txt',\n",
       " '96866.txt',\n",
       " '96929.txt',\n",
       " '97110.txt',\n",
       " '97235.txt',\n",
       " '97398.txt',\n",
       " '97445.txt',\n",
       " '97464.txt',\n",
       " '97967.txt',\n",
       " '98069.txt',\n",
       " '98128.txt',\n",
       " '98356.txt',\n",
       " '98482.txt',\n",
       " '98576.txt',\n",
       " '99053.txt',\n",
       " '99397.txt',\n",
       " '99581.txt',\n",
       " '99842.txt']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textos_test = os.listdir(PATH+'Test/')\n",
    "textos_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "41a89ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = open(PATH+'Test/76163.txt','r',encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c2ae643e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy buenos días:\n",
      "\n",
      " \n",
      "\n",
      "A veces, los políticos nos enfrascamos en grandes discusiones conceptuales e ideológicas y, otras veces, nos olvidamos de la vida cotidiana de las personas.\n",
      "\n",
      " \n",
      "\n",
      "Y hoy día, vamos a hablar de algo que afecta directamente la vida cotidiana de millones de chilenos, porque esta iniciativa de “Chile Sin Barreras”, que significa reemplazar los peajes físicos por peajes virtuales, electrónicos o digitales, le va a cambiar para mejor la vida a millones y millones de chilenos.\n",
      "\n",
      " \n",
      "\n",
      "El ministro ya lo dijo, este nuevo sistema de “Chile Sin Barreras”, significa, en primer lugar, ahorrar tiempo: 4 millones de horas/hombre con estas iniciativas; 15 millones de horas/hombre cuando lo extendamos a todo el país. Y ese mayor tiempo lo vamos a dedicar a lo que realmente importa: a la familia, a los amigos, a la cultura, al deporte, a la recreación, a la reflexión y, también, a la oración.\n",
      "\n",
      " \n",
      "\n",
      "Pero, además, va a significar un gran ahorro de recursos: 150 millones de dólares la primera etapa; 600 millones de dólares cuando lo tengamos implementado en todas las carreteras del país.\n",
      "\n",
      " \n",
      "\n",
      "Pero no es solamente tiempo y recursos. El que los autos y los camiones no tengan que detenerse, también va a hacer que nuestras carreteras sean más seguras, porque en esas detenciones se producen muchos accidentes. Y al ser carreteras más fluidas, van a ser carreteras más seguras y vamos también poder salvar vidas.\n",
      "\n",
      " \n",
      "\n",
      "Por eso, esta medida del “Chile Sin Barreras”, o Telepeaje, es una medida que va a mejorar la calidad de vida de los chilenos.\n",
      "\n",
      " \n",
      "\n",
      "Estamos partiendo por los accesos a Santiago. Es verdad, la Ruta 57, de Santiago a Los Andes; la 68, de Santiago a Valparaíso; la 78, de Santiago a San Antonio; la Ruta 5 Sur, hacia Talca; la Ruta 5 Sur, hacia el Norte; y también el radial que rodea a nuestra ciudad.\n",
      "\n",
      " \n",
      "\n",
      "Pero quiero decirlo muy claro, y que me escuchen muy bien mis compatriotas que viven en regiones: vamos a extender este sistema a todas las regiones de Chile y vamos a hacer que todas las carreteras de Chile tengan este nuevo sistema que va a mejorar la calidad de vida, va a ahorrar tiempo, va a ahorrar recursos y también va a hacer de nuestras carreteras, carreteras más seguras.\n",
      "\n",
      " \n",
      "\n",
      "Por eso, muchas veces, algunos piensan que este tipo de iniciativa no merece que se le destaque lo suficiente o atención de parte de los medios de comunicación. Yo creo que ésta es una buena noticia para todos nuestros compatriotas.\n",
      "\n",
      " \n",
      "\n",
      "E igual como estamos avanzando en “Chile Sin Barreras” y lo planteé en la Cuenta Pública, estamos avanzando en un montón de otras iniciativas para hacerles más fácil, más grata, la vida a nuestros compatriotas.\n",
      "\n",
      " \n",
      "\n",
      "Anunciamos a algunas de ellas, como extender los horarios de la atención de ciertos servicios, como los servicios bancarios o ciertos servicios públicos básicos como el Registro Civil o como el Servicio Médico Legal.\n",
      "\n",
      " \n",
      "\n",
      "Pero nuestra preocupación como Gobierno es una sola: hacer que la vida sea más plena y más feliz para todos nuestros compatriotas. Y para eso ¿se requieren grandes discusiones filosóficas? Sí, pero también se requiere tener los pies puestos en la tierra e ir simplificándoles los problemas a los chilenos.\n",
      "\n",
      " \n",
      "\n",
      "Quiero también destacar que “Chile Atiende”, que fue una iniciativa de nuestro primer Gobierno -y que después perdió fuerza y quedó en una especie de hibernación- está volviendo y con más fuerza que nunca. Nuestra meta es llegar a que el 80% de las gestiones y los trámites que los chilenos tenemos que hacer con el Estado, en lugar que nos signifiquen pérdidas de tiempo, de recursos, esperas y rabias, las podamos hacer desde la comodidad de nuestro teléfono celular o de nuestro computador.\n",
      "\n",
      " \n",
      "\n",
      "En el fondo, nunca olvidemos que el Gobierno está para facilitarles y no para hacerles más compleja la vida a las personas, y ése es norte que orienta este proyecto de “Chile Sin Barreras” y es el norte que orienta todas las actuaciones de nuestro Gobierno, hacer de Chile un país en que la gente pueda tener más oportunidades, más seguridades y vivir una vida más plena y más feliz.\n",
      "\n",
      " \n",
      "\n",
      "Muchas gracias.    \n"
     ]
    }
   ],
   "source": [
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f73feeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumir_texto(texto, lineas=0):\n",
    "    nlp_text=nlp(texto)\n",
    "    oraciones=[]\n",
    "    for sent in nlp_text.doc.sents:\n",
    "        oraciones.append(str(sent).replace('\\xa0','').replace('\\n',''))\n",
    "    oraciones_new=[EliminaNumeroYPuntuacion(Lematizar(oracion)) for oracion in oraciones]\n",
    "    features = [ObtenerEmbeddingOracion(modeloLSA, dim, oracion) for oracion in oraciones_new]\n",
    "    features = np.array(features)\n",
    "    print(f\"El texto contiene {len(features)} parrafos\")\n",
    "    if lineas==0 or lineas<0:\n",
    "        resumen=int(len(features)/2)\n",
    "    else:\n",
    "        resumen=lineas\n",
    "    print(f\"Se escogerán {resumen} parrafos para el resumen\")\n",
    "    similitudes = []\n",
    "    vector_promedio= features.mean(axis=0)\n",
    "    for feature in features: \n",
    "        similitud = 1-cosine(vector_promedio,feature)\n",
    "        similitudes.append(similitud)\n",
    "    print(f\"Las similitudes encontradas son \\n\" + '\\n'.join([str(x) for x in similitudes]))\n",
    "    df = pd.DataFrame({'oraciones':oraciones, 'oraciones_new':oraciones_new, 'similitudes':similitudes})\n",
    "    resumen_texto = [x for x in df.sort_values(by='similitudes').head(11).sort_index()['oraciones']]\n",
    "    return resumen_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "828dc0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El texto contiene 23 parrafos\n",
      "Se escogerán 11 parrafos para el resumen\n",
      "Las similitudes encontradas son \n",
      "0.996876869875476\n",
      "0.9968810640292879\n",
      "0.9922616368932953\n",
      "0.994984029951167\n",
      "0.9946886588390521\n",
      "0.9521718363663894\n",
      "0.9931162512799386\n",
      "0.9665074381381658\n",
      "0.9991035926054099\n",
      "0.9908520887640158\n",
      "0.9970724762501603\n",
      "0.9962173634581319\n",
      "0.9972847078127947\n",
      "0.9874634217477118\n",
      "0.9965814391107779\n",
      "0.9947077583232989\n",
      "0.992775397861699\n",
      "0.9679485324483886\n",
      "0.9936642730141638\n",
      "0.9950278012267574\n",
      "0.999232119850694\n",
      "0.9983147193726731\n",
      "0.7495089705091262\n"
     ]
    }
   ],
   "source": [
    "resumen=resumir_texto(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "92e5d1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ministro ya lo dijo, este nuevo sistema de “Chile Sin Barreras”, significa, en primer lugar, ahorrar tiempo: 4 millones de horas/hombre con estas iniciativas; 15 millones de horas/hombre cuando lo extendamos a todo el país.\n",
      "Pero, además, va a significar un gran ahorro de recursos: 150 millones de dólares la primera etapa; 600 millones de dólares cuando lo tengamos implementado en todas las carreteras del país.\n",
      "Pero no es solamente tiempo y recursos.\n",
      "El que los autos y los camiones no tengan que detenerse, también va a hacer que nuestras carreteras sean más seguras, porque en esas detencionesse producen muchos accidentes.\n",
      "Y al ser carreteras más fluidas, van a ser carreteras más seguras y vamos también poder salvar vidas.\n",
      "Estamos partiendo por los accesos a Santiago.\n",
      "Yo creo que ésta es una buena noticia para todos nuestros compatriotas.\n",
      "Pero nuestra preocupación como Gobierno es una sola: hacer que la vida sea más plena y más feliz para todos nuestros compatriotas.\n",
      "Y para eso ¿se requieren grandes discusiones filosóficas?\n",
      "Sí, pero también se requiere tener los pies puestos en la tierra e ir simplificándoles los problemas a los chilenos.\n",
      "Muchas gracias. \n"
     ]
    }
   ],
   "source": [
    "print( '\\n'.join(resumen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9092b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
