{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3065cf",
   "metadata": {
    "id": "Lu0zsf5HuOqR"
   },
   "source": [
    "**CLASIFICACIÓN DE SENTIMIENTOS CON LSA**\n",
    "\n",
    "Este programa utiliza  *Análisis Semántico Latente*  (LSA) para generar embeddings, que permitan realizar posteriormente una actividad de clasificación de sentimientos.\n",
    "\n",
    "La clasificación se realiza entrenando un modelo Bayesiano Gausiano.\n",
    "\n",
    "Primero, necesitamos instalar algunos paquetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11c06c21-04ea-413c-8e58-c9f88fa84253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9f7970",
   "metadata": {
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1632143158561,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "0KfmzU5tWWii"
   },
   "outputs": [],
   "source": [
    "import es_core_news_sm\n",
    "import os\n",
    "import pandas as pd\n",
    "import regex\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98e2e6",
   "metadata": {
    "id": "_KtZYqn4WZ8u"
   },
   "source": [
    "Definamos la función **CrearEspacioLSA(corpus,dim,NombreModelo)**, que  utiliza LSA para crear un *espacio semántico* de dimensiones reducidas (i.e., modelo vectorial que representa documentos y palabras), que se graba y luego puede ser cargado por otros programas. La función recibe la lista de documentos pre-procesados (**corpus**), el número de dimensiones a reducir (**dim**) vía [SVD](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491), y el nombre de la carpeta donde se grabará el modelo (**NombreModelo**).\n",
    "\n",
    "LSA utiliza la *descomposición de valores singulares* (SVD) para descomponer la matriz de frecuencias original vía **TfidfVectorizer** en tres matrices: $U$, $\\Sigma$ y $V^T$. Luego, las nuevas representaciones vectoriales en dimensiones reducidas (**dim**)  se reconstruyen  como:\n",
    "\n",
    "*   Representación de  términos: $U(dim) *  \\Sigma(dim)$\n",
    "\n",
    "*   Representación  de documentos: $\\Sigma(dim) * V^T(dim)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76370ad6",
   "metadata": {
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1632143167929,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "eCT-8_bXXJ-X"
   },
   "outputs": [],
   "source": [
    "def CrearModeloLSA(textos,dim,NombreModelo):\n",
    "  MatrizFrec = TfidfVectorizer()\n",
    "  tf = MatrizFrec.fit_transform(textos).T\n",
    "  U, Sigma, VT = svd(tf.toarray())\n",
    "  # Se realiza producto punto de matrices con las nuevas dimensiones\n",
    "  terms = np.dot(U[:,:dim], np.diag(Sigma[:dim]))\n",
    "  docs  = np.dot(np.diag(Sigma[:dim]), VT[:dim, :]).T \n",
    "  vocab = MatrizFrec.get_feature_names()\n",
    "  GrabarModeloLSA(NombreModelo, Sigma, terms, docs, vocab)\n",
    "\n",
    "def GrabarModeloLSA(NombreModelo,Sigma,terms,docs, vocab):\n",
    "   existe = os.path.isdir(NombreModelo)\n",
    "   if not existe:\n",
    "       os.mkdir(NombreModelo)\n",
    "   joblib.dump(Sigma,   NombreModelo +\"/\"+'sigma.pkl') \n",
    "   joblib.dump(terms,   NombreModelo +\"/\"+'terms.pkl') \n",
    "   joblib.dump(docs,    NombreModelo +\"/\"+'docs.pkl') \n",
    "   joblib.dump(vocab,   NombreModelo +\"/\"+'vocab.pkl') \n",
    "\n",
    "def CargarModeloLSA(NombreModelo):\n",
    "    terms   = joblib.load(NombreModelo+\"/\"+'terms.pkl')\n",
    "    vocab   = joblib.load(NombreModelo+\"/\"+'vocab.pkl')\n",
    "    modelo =  CrearDiccionario(terms,vocab)\n",
    "    return(modelo)\n",
    "def CrearDiccionario(lista_vectores,claves):\n",
    "   dicc = {}\n",
    "   for  v in range(0,len(claves)):\n",
    "      dicc[claves[v]] = lista_vectores[v]\n",
    "   return(dicc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1388504",
   "metadata": {
    "id": "ohFNag39i0bX"
   },
   "source": [
    "Un aspecto importante al utilizar LSA es determinar cuál es el número óptimo de dimensiones a reducir. Esto depende del tamaño del corpus utiizado para construir el modelo:\n",
    "\n",
    "1.   Si el tamaño es muy grande (i.e., varios GigaBytes de texto), lo adecuado es utilizar entre 200 a 300 dimensiones.\n",
    "2.   Si el tamaño  es pequeño, se puede elegir el número de dimensiones como el número de valores singulares de la matriz $\\Sigma$) que maximice la *importancia* (descartando el primer valor pues corelaciona con el largo del corpus). La importancia de un valor singular $x$ es simplemente $x^2$.\n",
    "\n",
    "Para visualizar la importancia de los valores singulares, podemos definir  la función **GraficarImportancia($\\Sigma$)**, que grafica los valores singulares (i.e., dimensiones) versus importancia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d5b2a13",
   "metadata": {
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1632143450389,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "Cr98HYUDjjbm"
   },
   "outputs": [],
   "source": [
    "def GraficarImportancia(Sigma):   \n",
    "    NumValores = np.arange(len(Sigma))\n",
    "    Importancia = [x**2 for x in Sigma]\n",
    "    plt.bar(NumValores,Importancia)\n",
    "    plt.ylabel('Importancia')\n",
    "    plt.xlabel('Valores Singulares')\n",
    "    plt.title('Importancia de Valores Singulares en SVD')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef79b0",
   "metadata": {
    "id": "ifJKRQJlhNlA"
   },
   "source": [
    "Luego, definimos algunas funciones utilitarias para \"limpieza\" y pre-procesamieto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b593fc4",
   "metadata": {
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1632144019054,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "NElqWSE4ijWn"
   },
   "outputs": [],
   "source": [
    " def PreProcesar(textos):\n",
    "    texto_limpio = []\n",
    "    for texto in textos:  \n",
    "        texto = Lematizar(texto)     \n",
    "        texto = EliminaNumeroYPuntuacion(texto)      \n",
    "        texto_limpio.append(texto)\n",
    "    return(texto_limpio)\n",
    "\n",
    "def Lematizar(oracion):\n",
    "   doc = nlp(oracion)\n",
    "   lemas = [token.lemma_ for token in doc]\n",
    "   return(\" \".join(lemas))  \n",
    "\n",
    "def EliminaNumeroYPuntuacion(oracion):\n",
    "    string_numeros = regex.sub(r'[\\”\\“\\¿\\°\\d+]','', oracion)\n",
    "    return ''.join(c for c in string_numeros if c not in punctuation)\n",
    "\n",
    "def Tokenizar(oracion):\n",
    "    doc = nlp(oracion)\n",
    "    tokens = [palabra.text for palabra in doc]\n",
    "    return(tokens)\n",
    "\n",
    "def CrearCorpus(PATH):\n",
    "  directorio = os.listdir(PATH)\n",
    "  corpus = []\n",
    "  doc_id = []  \n",
    "  for NombreArchivo  in directorio:\n",
    "     try:\n",
    "          texto = open(PATH+NombreArchivo,'r',encoding=\"utf-8\").read()\n",
    "          corpus.append(texto)\n",
    "          doc_id.append(NombreArchivo)\n",
    "     except IsADirectoryError:\n",
    "          texto = \"\"\n",
    "  return(corpus,doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dad95e",
   "metadata": {
    "id": "jKla5jkolKay"
   },
   "source": [
    "Una vez que generamos las nuevas representaciones vectoriales para documentos y términos, podríamos utilizar dichos  vectores para realizar diversas tareas tales como clustering, clasificación, comparación de documentos, etc.\n",
    "\n",
    "Como ejemplo, definamos la función **GraficarVectores(vocab,vectores)**, que toma vectores de documentos (o palabras), y el vocabulario, y grafica cada uno de los elementos en un gráfico 2-dimensional. Dado que los vectores contienen más de 2 dimensiones, en este ejemplo, sólo tomamos las 2 primeras. \n",
    "\n",
    "El gráfico le permitirá visualizar espacialmente la *cercanía* que existe entre documentos para posteriores análisis.\n",
    "\n",
    "Alternativamente, Ud. podría aplicar otros métodos de reducción dimensional para graficas las mejores 2 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1697008",
   "metadata": {
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1632143202214,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "3E1aQFpbmjTd"
   },
   "outputs": [],
   "source": [
    "def GraficarVectores(vocab,vectores):\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in vectores:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])   \n",
    "    plt.figure(figsize=(7, 7))   \n",
    "    plt.title(\"Distribución Espacial de Documentos\")\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(vocab[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fbd7a",
   "metadata": {
    "id": "11s2WiDpnGGl"
   },
   "source": [
    "Note que los vectores para documentos o palabras están indexados por posición y no por nombre, lo que podría dificultad el acceso. Para mejorar esto, podemos definir la función **CrearDiccionario(Vectores,vocabulario)**, que dado un conjunto de **vectores** (de documentos o palabras) y un **vocabulario**, genera un nuevo arreglo donde el índice es el nombre correspondiente: una palabra en el caso de vectores de palabras o un nombre de documento en el caso de vectores de documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f402f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObtenerEmbeddingOracion(modelo, dim, oracion):\n",
    "   Lista_enbeddings = []\n",
    "   Tokens = Tokenizar(oracion)\n",
    "   for w in Tokens:\n",
    "       # Verificar que la palabra w exista en el modelo\n",
    "       try:\n",
    "           modelo[w]\n",
    "       except KeyError:\n",
    "           continue\n",
    "       # Obtener vector de la palabra w\n",
    "       embedding = modelo[w]\n",
    "       Lista_enbeddings.append(embedding)\n",
    "   embedding_palabras = np.array(Lista_enbeddings)\n",
    "   if (len(embedding_palabras) > 0):\n",
    "        embedding_oracion = embedding_palabras.mean(axis=0)\n",
    "   else:\n",
    "        embedding_oracion = np.zeros(dim)\n",
    "   return(embedding_oracion) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fee028",
   "metadata": {
    "id": "bmoNbxoIn8YC"
   },
   "source": [
    "Ahora, realizamos nuestro programa principal donde ajustamos la ruta donde se encuentran los documentos del corpus e inicializamos los modelos de lenguaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6002ced",
   "metadata": {
    "executionInfo": {
     "elapsed": 2270,
     "status": "ok",
     "timestamp": 1632143210937,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "p4XxZz6JoD5I"
   },
   "outputs": [],
   "source": [
    "PATH = \"DiscursosOriginales/\"\n",
    "nlp = es_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a44bb",
   "metadata": {
    "id": "ZZlnKNVY10-C"
   },
   "source": [
    "Creamos un corpus a partir de los documentos de la carpeta de *PATH*. Luego, creamos el espacio semántico utilizando LSA y lo grabamos para recuperarlo posteriormente. Por ahora podemos suponer que vamos a reducir el espacio original a $dim=3$ ($dim$ debe ser menor que el número de documentos): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de20b13f",
   "metadata": {
    "executionInfo": {
     "elapsed": 1467,
     "status": "ok",
     "timestamp": 1632144025053,
     "user": {
      "displayName": "John Atkinson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg2bkbkmnbTiDRGZ-S-AD9kxpASaoVjtEAIDefWcA=s64",
      "userId": "05558035681160401394"
     },
     "user_tz": 180
    },
    "id": "uG1arANj2T2t",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DiscursosOriginales/Train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/2jnszg1d0llfrs9_1_8prq9r0000gn/T/ipykernel_4951/484983026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodeloLSA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCargarModeloLSA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mi_lsa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlista_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrearCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtextos\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mPreProcesar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mCrearModeloLSA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mi_lsa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/sh/2jnszg1d0llfrs9_1_8prq9r0000gn/T/ipykernel_4951/2823594353.py\u001b[0m in \u001b[0;36mCrearCorpus\u001b[0;34m(PATH)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCrearCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m  \u001b[0mdirectorio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m  \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m  \u001b[0mdoc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DiscursosOriginales/Train/'"
     ]
    }
   ],
   "source": [
    "dim =  200\n",
    "if os.path.isdir(\"mi_lsa\"):\n",
    "    modeloLSA = CargarModeloLSA(\"mi_lsa\")\n",
    "else:\n",
    "    corpus, lista_docs = CrearCorpus(PATH+'Train/')\n",
    "    textos  = PreProcesar(corpus)\n",
    "    CrearModeloLSA(textos,dim,\"mi_lsa\")\n",
    "    modeloLSA = CargarModeloLSA(\"mi_lsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "014c79a3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DiscursosOriginales/Test/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/2jnszg1d0llfrs9_1_8prq9r0000gn/T/ipykernel_4951/1984802126.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtextos_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Test/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtextos_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DiscursosOriginales/Test/'"
     ]
    }
   ],
   "source": [
    "textos_test = os.listdir(PATH+'Test/')\n",
    "textos_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41a89ee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DiscursosOriginales/Test/76163.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/2jnszg1d0llfrs9_1_8prq9r0000gn/T/ipykernel_4951/186822272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtexto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Test/76163.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DiscursosOriginales/Test/76163.txt'"
     ]
    }
   ],
   "source": [
    "texto = open(PATH+'Test/76163.txt','r',encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2ae643e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/2jnszg1d0llfrs9_1_8prq9r0000gn/T/ipykernel_4951/7217826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'texto' is not defined"
     ]
    }
   ],
   "source": [
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f73feeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumir_texto(texto, lineas=0):\n",
    "    nlp_text=nlp(texto)\n",
    "    oraciones=[]\n",
    "    for sent in nlp_text.doc.sents:\n",
    "        oraciones.append(str(sent).replace('\\xa0','').replace('\\n',''))\n",
    "    oraciones_new=[EliminaNumeroYPuntuacion(Lematizar(oracion)) for oracion in oraciones]\n",
    "    features = [ObtenerEmbeddingOracion(modeloLSA, dim, oracion) for oracion in oraciones_new]\n",
    "    features = np.array(features)\n",
    "    print(f\"El texto contiene {len(features)} parrafos\")\n",
    "    if lineas==0 or lineas<0:\n",
    "        resumen=int(len(features)/2)\n",
    "    else:\n",
    "        resumen=lineas\n",
    "    print(f\"Se escogerán {resumen} parrafos para el resumen\")\n",
    "    similitudes = []\n",
    "    vector_promedio= features.mean(axis=0)\n",
    "    for feature in features: \n",
    "        similitud = 1-cosine(vector_promedio,feature)\n",
    "        similitudes.append(similitud)\n",
    "    print(f\"Las similitudes encontradas son \\n\" + '\\n'.join([str(x) for x in similitudes]))\n",
    "    df = pd.DataFrame({'oraciones':oraciones, 'oraciones_new':oraciones_new, 'similitudes':similitudes})\n",
    "    resumen_texto = [x for x in df.sort_values(by='similitudes').head(11).sort_index()['oraciones']]\n",
    "    return resumen_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "828dc0ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/2jnszg1d0llfrs9_1_8prq9r0000gn/T/ipykernel_4951/3391857768.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresumen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresumir_texto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'texto' is not defined"
     ]
    }
   ],
   "source": [
    "resumen=resumir_texto(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92e5d1d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resumen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sh/2jnszg1d0llfrs9_1_8prq9r0000gn/T/ipykernel_4951/4147723573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresumen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'resumen' is not defined"
     ]
    }
   ],
   "source": [
    "print( '\\n'.join(resumen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9092b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
